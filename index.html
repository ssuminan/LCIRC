<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs">
  <meta name="keywords" content="LCIRC, QD-LCIRC, long-form context">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png"> 

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
  <!-- KaTex -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.css">
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "$", right: "$", display: false}
        ]
      });
    });
  </script>
  <!-- KaTex -->
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs</h1>
          <div class="is-size-3 publication-authors">
            <img src="./static/images/acl-logo.png" alt="NAACL Logo" style="height: 40px; vertical-align: middle;">
            <b>NAACL 2025</b>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Sumin An<sup>1</sup>,</span>
            <span class="author-block">Junyoung Sung<sup>1</sup>,</span>
            <span class="author-block">Wonpyo Park<sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://parkchanjun.github.io/">Chanjun Park</a><sup>1,&dagger;</sup>,</span>
            <span class="author-block">
              <a href="https://phseo.github.io/">Paul Hongsuck Seo</a><sup>1,&dagger;
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Korea University,</span>
            <span class="author-block"><sup>2</sup>Google</span>
            <span class="eql-cntrb"><small><br><sup>&dagger;</sup>Co-corresponding Authors</small></span>
          </div>
          <div style="display: flex; justify-content: center; align-items: center;">
            <a href="https://www.korea.edu/sites/en/index.do" target="_blank">
              <img src="./static/images/ku-logo.png" alt="korea" style="height: 50px; margin-right: 60px;">
            </a>
            <a href="https://miil.korea.ac.kr/" target="_blank">
              <img src="./static/images/miil.png" alt="miil" style="height: 54px; margin-right: 50px;">
            </a>
            <img src="./static/images/Google_2015_logo.svg.webp" alt="google" style="height: 50px;">
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2502.06139"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.06139"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/ssuminan/LCIRC"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ssumin/fineweb-lqa"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" 
                         alt="Hugging Face" style="width: 20px; height: 20px;">
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While large language models (LLMs) excel in generating coherent and contextually rich outputs,
            their capacity to efficiently handle long-form contexts is limited by fixed-length position embeddings.
            Additionally, the computational cost of processing long sequences increases quadratically,
            making it challenging to extend context length.
          </p>
          <p>
            To address these challenges, we propose Long-form Context Injection with Recurrent Compression (LCIRC),
            a method that enables the efficient processing long-form sequences beyond the model's length limit
            through recurrent compression without retraining the entire model.
          </p>
          <p>
            We further introduce query dependent context modeling, which selectively compresses query-relevant
            information, ensuring that the model retains the most pertinent content. Our empirical results
            demonstrate that Query Dependent LCIRC (QD-LCIRC) significantly improves LLM's ability to manage
            extended contexts, making it well-suited for tasks that require both comprehensive context
            understanding and query relevance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <img src="./static/images/lcirc.png" class="center"/>
        <div class="content has-text-justified">
          <p>
            <b>The overall process of the proposed Long-form Context Injection with Recurrent Compression (LCIRC).</b>
            LCIRC comprises two components: Recurrent Context Compression (left) and Compressed Context Injection (right).
            In the $i$-th step of Recurrent Context Compression, the previously compressed features $\mathbf{h}^{(i-1)}$ and
            the segment embeddings $\mathbf{s}_{i}$ are fed into the Perceiver module as query and input features, respectively.
            The compressed features $\mathbf{h}^{(i)}$ are then generated and reinjected as query features for the subsequent
            recurrence step. The initial query features $\mathbf{h}^{(0)}$ are learnable parameters. In Compressed Context Injection,
            the concatenated compressed features $\mathbf{h}$ serve as input to the Gated Cross Attention layer.
            Layers indicated with a fire symbol represent trained layers, while layers marked with a snow symbol denote frozen layers.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Training</h2>
        <img src="./static/images/ssbptt.png" class="center"/>
        <div class="content has-text-justified">
          <p>
            <b>Comparisons of the proposed Selective State BPTT with vanilla and truncated BPTT.</b>
            Green boxes represent timesteps where gradients are computed in BPTT whereas the light green ones indicate the timesteps without gradient computation. 
            Finally, dotted red lines illustrate the gradient flows.
            (a) Vanilla BPTT computes the full gradients through the entire timesteps in recurrence but is computationally infeasible with a large $N$.
            The gradients for $\mathbf{h}^{(i)}$ receives upstream gradients both through the recurrent connection and through the direct connection from $\mathbf{h}$. 
            (b) Truncated BPTT backprobagates gradients to the last $T$ timesteps only significantly reducing computational costs.
            However, it does not transfer gradient flows to timesteps further than $T$ (marked with light green color) and fails to learn long-term QD modeling.
            (c) Our proposed Selective State BPTT selects several random timesteps and transfer gradient flows directly through the direct connection from $\mathbf{h}$,
            which enables efficient learning of long-term QD modeling capabilities.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Results</h2>
        <img src="./static/images/result1.png" class="center"/>
        <div class="content has-text-justified">
          <p>
            <b>Per-task performance on InfiniteBench and LongBench.</b>
            The following abbreviations are used: <b>NQA</b> denotes NarrativeQA, <b>MFQA</b> represents MultiFieldQA-en,
            <b>HQA</b> refers to HotpotQA, <b>2WQA</b> to 2WikiMQA, and <b>MSQ</b> to MuSiQue. <b>Avg</b> indicates the average score across all subtasks within respective benchmarks.
            <b>FW-LQA</b> indicates whether the model is fine-tuned on FineWeb-LQA.
            Our QD-LCIRC consistently outperforms competing methods, achieving the highest average score by incorporating query dependent modeling, as indicated in the <b>QD</b> column.
          </p>
        </div>
        <img src="./static/images/result2.png" class="center"/>
        <div class="content has-text-justified">
          <p>
            <b>Per-task performance on L-Eval.</b>
            The following abbreviations are used: <b>CS</b> denotes Coursera, <b>QALIT</b> refers to QuALITY, <b>SF</b> represents SFiction, <b>LFQA</b> refers to LongFQA, 
            and <b>NQA</b> to NarrativeQA. <b>Avg</b> indicates the mean performance score across all subtasks within the respective benchmark. <b>FW-LQA</b> indicates whether 
            the model has been fine-tuned on FineWeb-LQA, while <b>QD</b> denotes whether query dependent modeling.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{an2025lcirc,
  title={LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs},
  author={An, Sumin and Sung, Junyoung and Park, Wonpyo and Park, Chanjun and Seo, Paul Hongsuck},
  journal={arXiv preprint arXiv:2502.06139},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
